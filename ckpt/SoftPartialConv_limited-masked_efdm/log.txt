/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/log/SoftPartialConv_limited-masked_efdmlog.txt
cuda: cuda:0
Version: SoftPartialConv_limited-masked_efdm
train mode
content dir: /mnt/sda/Datasets/Detection/COCO/train2017
style dir: /mnt/sda/Datasets/style_image/AlphaStyle/alpha_WikiArt_AllInOne2/train_alpha
class Config:
    phase = 'train'         # You must change the phase into train/test/style_blending
    # phase = 'test'         # You must change the phase into train/test/style_blending
    train_continue = 'off'  # on / off

    data_num = 60000        # Maximum # of training data

    content_dir = '/mnt/sda/Datasets/Detection/COCO/train2017'
    # style_dir = '/mnt/sda/Dataset/style_image/dunhuang_style/crop_256/main_white/origin'
    # mask_dir = '/mnt/sda/Dataset/style_image/dunhuang_style/crop_256/main_white/mask'
    style_dir = '/mnt/sda/Datasets/style_image/AlphaStyle/alpha_WikiArt_AllInOne2/train_alpha'
    # mask_dir = '/mnt/sda/Dataset/style_image/dunhuang_style/crop_256/main_white/mask'
    cuda_device = 'cuda:0'
    
    # file_n = 'SoftPartialConv_limited-800000_iter' # 新训练时, 此处需要修改. 测试时, 也需要修改此处
    file_n = 'SoftPartialConv_limited-masked_efdm' # 新训练时, 此处需要修改. 测试时, 也需要修改此处
    log_file_path = '/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/log/' + file_n + 'log.txt'
    log_dir = './log/' + file_n
    ckpt_dir = './ckpt/' + file_n
    img_dir = './Generated_images/' + file_n
    
    if phase == 'test':
        multi_to_multi = True
        test_content_size = 256
        test_style_size = 256

        mod = 'main' # main 表示迁移风格图像, back表示迁移背景图像, 当该值为main时, 将使用主体图像进行风格迁移；当该值为 back 时, 将使用背景图像进行风格迁移. 同时, 该值还用于创建保存图像的文件名.
        content_dir = '/mnt/sdb/zxt/3_code_area/code_develop/PartialConv_AesFA/input/contents/alpha'
        # style_dir = '/mnt/sdb/zxt/3_code_area/code_develop/PartialConv_AesFA/input/styles/dunhuang/alpha'  # 敦煌
        style_dir = '/mnt/sdb/zxt/3_code_area/code_develop/PartialConv_AesFA/input/styles/wikiart/alpha'    # wikiart
        style_dir = '/mnt/sdb/zxt/3_code_area/code_develop/PartialConv_AesFA/input/styles/wikiart/alpha'    # wikiart


        # mask_dir = '/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/imgs/masks' + '/' + mod
        # style_dir = '/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/imgs/styles/origin'
        # mask_dir = '/mnt/sda/zxt/3_code_area/code_develop/PartialConv_AesFA/imgs/styles/mask'

        ckpt_iter = 160000
        ckpt_epoch = 22
        ckpt_name = 'model_iter_' + str(ckpt_iter) + '_epoch_' + str(ckpt_epoch) + '.pth'
         
        img_dir = './output/'+file_n + '/' + ckpt_name.split('.')[0] + '_' + str(test_content_size)+  '/'

    elif phase == 'style_blending':
        blend_load_size = 256
        blend_dir = './blendingDataset/'
        content_img = blend_dir + str(blend_load_size) + '/A.jpg'
        style_high_img = blend_dir + str(blend_load_size) + '/B.jpg'
        style_low_img = blend_dir + str(blend_load_size) + '/C.jpg'
        img_dir = './output/'+file_n+'_blending_' + str(blend_load_size)
        
    # VGG pre-trained model
    vgg_model = './vgg_normalised.pth'

    ## basic parameters
    n_iter = 160000 
    batch_size = 1
    lr = 0.0001
    lr_policy = 'step'
    lr_decay_iters = 50
    beta1 = 0.0

    # preprocess parameters
    load_size = 512
    crop_size = 256

    # model parameters
    input_nc = 3         # of input image channel
    nf = 64              # of feature map channel after Encoder first layer
    output_nc = 3        # of output image channel
    style_kernel = 3     # size of style kernel
    
    # Octave Convolution parameters
    alpha_in = 0.5       # input ratio of low-frequency channel
    alpha_out = 0.5      # output ratio of low-frequency channel
    freq_ratio = [1, 1]  # [high, low] ratio at the last layer

    # Loss ratio
    lambda_percept = 1.0
    lambda_perc_cont = 1.0
    lambda_perc_style = 10.0
    lambda_const_style = 5.0

    # Else
    norm = 'instance'
    init_type = 'normal'
    init_gain = 0.02
    no_dropout = 'store_true'
    num_workers = 4

Train:  60000 images:  60000 x 1 (batch size) = 60000
# of parameter: 22470831
parameters of networks: {'netE': 6645888, 'netS': 6645888, 'netG': 5673315, 'vgg_loss': 3505740}
len(o2): 2, o2[0].shape: torch.Size([1, 32, 128, 128]), o2[1].shape:torch.Size([1, 32, 64, 64])
len(o3): 2, o3[0].shape: torch.Size([1, 32, 128, 128]), o3[1].shape:torch.Size([1, 32, 64, 64])
len(o4): 2, o4[0].shape: torch.Size([1, 64, 128, 128]), o4[1].shape:torch.Size([1, 64, 64, 64])
len(o5): 2, o5[0].shape: torch.Size([1, 64, 128, 128]), o5[1].shape:torch.Size([1, 64, 64, 64])
len(o6): 2, o6[0].shape: torch.Size([1, 64, 128, 128]), o6[1].shape:torch.Size([1, 64, 64, 64])
len(o7): 2, o7[0].shape: torch.Size([1, 64, 128, 128]), o7[1].shape:torch.Size([1, 64, 64, 64])
len(o8): 2, o8[0].shape: torch.Size([1, 64, 64, 64]), o8[1].shape:torch.Size([1, 64, 32, 32])
len(o10): 2, o10[0].shape: torch.Size([1, 128, 64, 64]), o10[1].shape:torch.Size([1, 128, 32, 32])
len(o12): 2, o12[0].shape: torch.Size([1, 128, 64, 64]), o12[1].shape:torch.Size([1, 128, 32, 32])
len(o14): 2, o14[0].shape: torch.Size([1, 128, 64, 64]), o14[1].shape:torch.Size([1, 128, 32, 32])
len(o16): 2, o16[0].shape: torch.Size([1, 256, 64, 64]), o16[1].shape:torch.Size([1, 256, 32, 32])
len(o17): 2, o17[0].shape: torch.Size([1, 256, 64, 64]), o17[1].shape:torch.Size([1, 256, 32, 32])
len(o2): 2, o2[0].shape: torch.Size([1, 32, 128, 128]), o2[1].shape:torch.Size([1, 32, 64, 64])
len(o3): 2, o3[0].shape: torch.Size([1, 32, 128, 128]), o3[1].shape:torch.Size([1, 32, 64, 64])
len(o4): 2, o4[0].shape: torch.Size([1, 64, 128, 128]), o4[1].shape:torch.Size([1, 64, 64, 64])
len(o5): 2, o5[0].shape: torch.Size([1, 64, 128, 128]), o5[1].shape:torch.Size([1, 64, 64, 64])
len(o6): 2, o6[0].shape: torch.Size([1, 64, 128, 128]), o6[1].shape:torch.Size([1, 64, 64, 64])
len(o7): 2, o7[0].shape: torch.Size([1, 64, 128, 128]), o7[1].shape:torch.Size([1, 64, 64, 64])
len(o8): 2, o8[0].shape: torch.Size([1, 64, 64, 64]), o8[1].shape:torch.Size([1, 64, 32, 32])
len(o10): 2, o10[0].shape: torch.Size([1, 128, 64, 64]), o10[1].shape:torch.Size([1, 128, 32, 32])
len(o12): 2, o12[0].shape: torch.Size([1, 128, 64, 64]), o12[1].shape:torch.Size([1, 128, 32, 32])
len(o14): 2, o14[0].shape: torch.Size([1, 128, 64, 64]), o14[1].shape:torch.Size([1, 128, 32, 32])
len(o16): 2, o16[0].shape: torch.Size([1, 256, 64, 64]), o16[1].shape:torch.Size([1, 256, 32, 32])
len(o17): 2, o17[0].shape: torch.Size([1, 256, 64, 64]), o17[1].shape:torch.Size([1, 256, 32, 32])
content_A[0].shape:torch.Size([1, 256, 64, 64])
content_A[1].shape:torch.Size([1, 256, 32, 32])
style_B[0].shape:torch.Size([1, 256, 3, 3])
style_B[1].shape:torch.Size([1, 256, 3, 3])
torch.Size([1, 3, 512, 512])
torch.Size([1, 3, 256, 256])
len(o2): 2, o2[0].shape: torch.Size([1, 32, 256, 256]), o2[1].shape:torch.Size([1, 32, 128, 128])
len(o3): 2, o3[0].shape: torch.Size([1, 32, 256, 256]), o3[1].shape:torch.Size([1, 32, 128, 128])
len(o4): 2, o4[0].shape: torch.Size([1, 64, 256, 256]), o4[1].shape:torch.Size([1, 64, 128, 128])
len(o5): 2, o5[0].shape: torch.Size([1, 64, 256, 256]), o5[1].shape:torch.Size([1, 64, 128, 128])
len(o6): 2, o6[0].shape: torch.Size([1, 64, 256, 256]), o6[1].shape:torch.Size([1, 64, 128, 128])
len(o7): 2, o7[0].shape: torch.Size([1, 64, 256, 256]), o7[1].shape:torch.Size([1, 64, 128, 128])
len(o8): 2, o8[0].shape: torch.Size([1, 64, 128, 128]), o8[1].shape:torch.Size([1, 64, 64, 64])
len(o10): 2, o10[0].shape: torch.Size([1, 128, 128, 128]), o10[1].shape:torch.Size([1, 128, 64, 64])
len(o12): 2, o12[0].shape: torch.Size([1, 128, 128, 128]), o12[1].shape:torch.Size([1, 128, 64, 64])
len(o14): 2, o14[0].shape: torch.Size([1, 128, 128, 128]), o14[1].shape:torch.Size([1, 128, 64, 64])
len(o16): 2, o16[0].shape: torch.Size([1, 256, 128, 128]), o16[1].shape:torch.Size([1, 256, 64, 64])
len(o17): 2, o17[0].shape: torch.Size([1, 256, 128, 128]), o17[1].shape:torch.Size([1, 256, 64, 64])
len(o2): 2, o2[0].shape: torch.Size([1, 32, 256, 256]), o2[1].shape:torch.Size([1, 32, 128, 128])
len(o3): 2, o3[0].shape: torch.Size([1, 32, 256, 256]), o3[1].shape:torch.Size([1, 32, 128, 128])
len(o4): 2, o4[0].shape: torch.Size([1, 64, 256, 256]), o4[1].shape:torch.Size([1, 64, 128, 128])
len(o5): 2, o5[0].shape: torch.Size([1, 64, 256, 256]), o5[1].shape:torch.Size([1, 64, 128, 128])
len(o6): 2, o6[0].shape: torch.Size([1, 64, 256, 256]), o6[1].shape:torch.Size([1, 64, 128, 128])
len(o7): 2, o7[0].shape: torch.Size([1, 64, 256, 256]), o7[1].shape:torch.Size([1, 64, 128, 128])
len(o8): 2, o8[0].shape: torch.Size([1, 64, 128, 128]), o8[1].shape:torch.Size([1, 64, 64, 64])
len(o10): 2, o10[0].shape: torch.Size([1, 128, 128, 128]), o10[1].shape:torch.Size([1, 128, 64, 64])
len(o12): 2, o12[0].shape: torch.Size([1, 128, 128, 128]), o12[1].shape:torch.Size([1, 128, 64, 64])
len(o14): 2, o14[0].shape: torch.Size([1, 128, 128, 128]), o14[1].shape:torch.Size([1, 128, 64, 64])
len(o16): 2, o16[0].shape: torch.Size([1, 256, 128, 128]), o16[1].shape:torch.Size([1, 256, 64, 64])
len(o17): 2, o17[0].shape: torch.Size([1, 256, 128, 128]), o17[1].shape:torch.Size([1, 256, 64, 64])
self.real_content shape: torch.Size([1, 3, 256, 256])
self.real_style shape: torch.Size([1, 3, 256, 256])
self.trs_AtoB shape: torch.Size([1, 3, 512, 512])
